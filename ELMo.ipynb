{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELMo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nuccy90/Master_thesis/blob/master/ELMo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "x_jNHL_tNEEo",
        "colab_type": "code",
        "outputId": "191069c5-a714-4d0b-8ab3-f3fe0f977b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3046
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/c8/10342a6068a8d156a5947e03c95525d559e71ad62de0f2585ab922e14533/allennlp-0.8.3-py3-none-any.whl (5.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.18.4)\n",
            "Collecting numpydoc>=0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/a8/b4706a6270f0475541c5c1ee3373c7a3b793936ec1f517f1a1dab4f896c0/numpydoc-0.8.0.tar.gz\n",
            "Collecting word2number>=1.1 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.1.post2)\n",
            "Collecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.123)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Collecting awscli>=1.11.91 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/91/a5b1643bb44e3fece79ab6f7bfc9c40793be42b8db31e0cbfdbb5d3cff99/awscli-1.16.137-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 15.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
            "Collecting unidecode (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 31.5MB/s \n",
            "\u001b[?25hCollecting ftfy (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 24.6MB/s \n",
            "\u001b[?25hCollecting flask-cors>=3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.14.6)\n",
            "Collecting conllu==0.11 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Collecting parsimonious>=0.8.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 24.3MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/dc/3abd3971869a741d7acdba166d71d4f9366b6b53028dfd56f95de356af0f/jsonnet-0.12.1.tar.gz (240kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 26.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Collecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.6)\n",
            "Requirement already satisfied: spacy<2.2,>=2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0.18)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Collecting moto>=1.3.4 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/40/cec89fa5c13108eb1c8de435633f8b7639e0e43fcbcdc8ac52633efeeabe/moto-1.3.7-py2.py3-none-any.whl (552kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 28.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.2)\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/76/89dd44458eb976347e5a6e75eb79fecf8facd46c1ce259bad54e0044ea35/tensorboardX-1.6-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 32.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.20.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.6)\n",
            "Requirement already satisfied: sphinx>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.123 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.12.123)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->allennlp) (1.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (6.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (40.8.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2018.1.10)\n",
            "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Collecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.3.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (1.35)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (6.12.1)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (0.2.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (2.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (2.0.1)\n",
            "Collecting jsondiff==1.1.1 (from moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/5f/13e28a2f9abeda2ffb3f44f2f809b01b52bc02cdb63816e05b8c9cbbdfc5/jsondiff-1.1.1.tar.gz\n",
            "Collecting python-jose<3.0.0 (from moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.0.0)\n",
            "Collecting docker>=2.5.1 (from moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/68/c3afca1a5aa8d2997ec3b8ee822a4d752cf85907b321f07ea86888545152/docker-3.7.2-py2.py3-none-any.whl (134kB)\n",
            "\u001b[K    100% |████████████████████████████████| 143kB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto>=2.36.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.49.0)\n",
            "Requirement already satisfied: werkzeug in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (0.15.1)\n",
            "Collecting aws-xray-sdk<0.96,>=0.93 (from moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.0MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.3.0 (from moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 15.4MB/s \n",
            "\u001b[?25hCollecting pyaml (from moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/e1/1523fb1dab744e2c6b1f02446f2139a78726c18c062a8ddd53875abb20f8/pyaml-18.11.0-py2.py3-none-any.whl\n",
            "Collecting xmltodict (from moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.2.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.4.3.2)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.9.0.1)\n",
            "Collecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/b4/7d007370568d98822c833e0d0e804417620865710dc8a5830bbed58328d1/pycryptodome-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 9.7MB 1.1MB/s \n",
            "\u001b[?25hCollecting ecdsa<1.0 (from python-jose<3.0.0->moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f4/73669d51825516ce8c43b816c0a6b64cd6eb71d08b99820c00792cb42222/ecdsa-0.13-py2.py3-none-any.whl (86kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 27.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: future<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<3.0.0->moto>=1.3.4->allennlp) (0.16.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock->moto>=1.3.4->allennlp) (5.1.3)\n",
            "Collecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.32.0 (from docker>=2.5.1->moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 31.0MB/s \n",
            "\u001b[?25hCollecting jsonpickle (from aws-xray-sdk<0.96,>=0.93->moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/dc/12/8c44eabb501e2bc0aec0dd152b328074d98a50968d3a02be28f6037f0c6a/jsonpickle-1.1-py2.py3-none-any.whl\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.3.0->moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 30.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (1.12.2)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.9.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3.0->moto>=1.3.4->allennlp) (2.19)\n",
            "Building wheels for collected packages: numpydoc, word2number, overrides, parsimonious, jsonnet, jsondiff\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/55/7f/3e25d754760ccd62d6796e5b2cfe25629346f52ea00753d549\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f0/47/51/a178b15274ed0db775a1ae9c799ce31e511609c3ab75a7dec5\n",
            "  Building wheel for jsondiff (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/68/08/07/69d839606fb7fdc778fa86476abc0a864693d45969a0c1936c\n",
            "Successfully built numpydoc word2number overrides parsimonious jsonnet jsondiff\n",
            "\u001b[31mawscli 1.16.137 has requirement botocore==1.12.127, but you'll have botocore 1.12.123 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpydoc, word2number, flaky, pytorch-pretrained-bert, overrides, rsa, colorama, awscli, unidecode, ftfy, flask-cors, conllu, parsimonious, jsonnet, responses, jsondiff, pycryptodome, ecdsa, python-jose, docker-pycreds, websocket-client, docker, jsonpickle, aws-xray-sdk, asn1crypto, cryptography, pyaml, xmltodict, moto, tensorboardX, allennlp\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "Successfully installed allennlp-0.8.3 asn1crypto-0.24.0 aws-xray-sdk-0.95 awscli-1.16.137 colorama-0.3.9 conllu-0.11 cryptography-2.6.1 docker-3.7.2 docker-pycreds-0.4.0 ecdsa-0.13 flaky-3.5.3 flask-cors-3.0.7 ftfy-5.5.1 jsondiff-1.1.1 jsonnet-0.12.1 jsonpickle-1.1 moto-1.3.7 numpydoc-0.8.0 overrides-1.9 parsimonious-0.8.1 pyaml-18.11.0 pycryptodome-3.8.0 python-jose-2.0.2 pytorch-pretrained-bert-0.6.1 responses-0.10.6 rsa-3.4.2 tensorboardX-1.6 unidecode-1.0.23 websocket-client-0.56.0 word2number-1.1 xmltodict-0.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tSfnXhMPGqU-",
        "colab_type": "code",
        "outputId": "0f48c017-c88f-4383-9397-adca99799336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import itertools\n",
        "import json\n",
        "import nltk\n",
        "from allennlp.commands.elmo import ElmoEmbedder\n",
        "from tensorflow import keras\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zatLzk6LbDwB",
        "colab_type": "code",
        "outputId": "74054863-cfbf-47a8-c696-11bfd40439a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "h9mGeE6rs6Lq",
        "outputId": "447bcde6-ead8-4bde-e71d-4aaaf2af162a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "M8jL3EpfGxq7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_data(name):\n",
        "    \n",
        "    df = pd.read_csv(name)\n",
        "    df = df.dropna(subset=['Text'])\n",
        "    df['Title'] = df['Title'].fillna('')\n",
        "    df = df[df[\"Text\"] != \"  \"]\n",
        "    df = df[df['Title'] + df['Text'] != '']\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B_8oJlSNG-9f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cleanText(text):\n",
        "    \n",
        "    #sw = stopwords.words('english')[35:]\n",
        "    text = re.sub(r'\\|\\|\\|', r' ', text)\n",
        "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
        "    text = re.sub(r'\\d{6,}', r'<NUM>', text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iiRd2CyLYB7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_elmo_embeddings(document, elmo):\n",
        "    \n",
        "    sentences = sent_tokenize(document)\n",
        "    embeddings = []\n",
        "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    \n",
        "    #here you need to decide if you want to cut to 100 words\n",
        "    #before you do the embeddings\n",
        "    \n",
        "    for elmo_embedding in elmo.embed_sentences(tokens):  \n",
        "        # Average the 3 layers returned from ELMo\n",
        "        avg_elmo_embedding = np.average(elmo_embedding, axis=0)\n",
        "             \n",
        "        embeddings.extend(avg_elmo_embedding)\n",
        "        \n",
        "            \n",
        "    return embeddings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GiCaqr_MILsy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get vectors\n",
        "\n",
        "def get_vectors(docs, elmo):\n",
        "\n",
        "    X = []\n",
        "\n",
        "    for doc in docs:\n",
        "    \n",
        "        doc_embeddings = create_elmo_embeddings(doc, elmo)\n",
        "        X.append(doc_embeddings)\n",
        "        \n",
        "    return X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ox0vG4iZHDiT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sort_by_date(df):\n",
        "\n",
        "    di = {}        \n",
        "    for index, row in df.iterrows():\n",
        "\n",
        "        sub_id = row[\"Subject\"]\n",
        "        diagnosis = row[\"Diagnosis\"]\n",
        "        if sub_id in di:\n",
        "            di[sub_id][1].append(row[\"Text\"])\n",
        "            di[sub_id][2].append(row[\"Date\"])\n",
        "        else:\n",
        "            di[sub_id] = [diagnosis,[row[\"Text\"]], [row[\"Date\"]]]\n",
        "\n",
        "    for key in di:\n",
        "        list_of_datetimes = [datetime.strptime(x, ' %Y-%m-%d %H:%M:%S ') for x in di[key][2]]\n",
        "\n",
        "        lists = sorted(zip(*[list_of_datetimes, di[key][1]]))\n",
        "        dates, texts = list(zip(*lists))\n",
        "\n",
        "        di[key][1] = texts\n",
        "        di[key][2] = dates\n",
        "        \n",
        "    return di"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wNHZRTHgHJy0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_texts(di, model):\n",
        "    print(\"Starting prediction...\")\n",
        "    pred_dict = {}\n",
        "    \n",
        "    elmo = ElmoEmbedder(cuda_device = 0)\n",
        "    \n",
        "    def predict_scores(texts, model = model, elmo = elmo):\n",
        "    \n",
        "        #add score to dictionary\n",
        "        x_texts = get_vectors(texts, elmo)\n",
        "        x_texts = sequence.pad_sequences(x_texts, maxlen=100)\n",
        "        if len(texts) == 1:\n",
        "            print(x_texts)\n",
        "            scores = model.predict(x_texts)[0]\n",
        "        else:\n",
        "            scores = model.predict(x_texts)\n",
        "            \n",
        "        return scores\n",
        "    \n",
        "    for sub_id in di:\n",
        "        texts = di[sub_id][1]\n",
        "        pred_dict[sub_id] = predict_scores(texts)\n",
        "    \n",
        "    print(\"Prediction done!\")\n",
        "    return pred_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Xy7-0jJHO7x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_train_vectors(pred_dict, di):\n",
        "    \n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for i in range(1,2000):\n",
        "        for sub_id in pred_dict:\n",
        "        \n",
        "            if i >= len(pred_dict[sub_id]):\n",
        "                pass\n",
        "            else:\n",
        "                seen = pred_dict[sub_id][:i]\n",
        "                avg = np.mean(seen)\n",
        "                sd = np.std(seen)\n",
        "                top_n = int(round((20*i)/100))\n",
        "                topn_avg = np.mean(np.sort(seen)[top_n:])\n",
        "                bottomn_avg = np.mean(np.sort(seen)[:top_n+1])\n",
        "                diff = topn_avg - bottomn_avg\n",
        "                n_texts = (i-1)/(1999-1)\n",
        "            \n",
        "                x = np.array([n_texts,avg,sd,topn_avg,diff])\n",
        "                x_train.append(x)\n",
        "                y_train.append(di[sub_id][0])\n",
        "                \n",
        "    return np.array(x_train), np.array(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zT9hOuTHHWEV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_test_vectors(pred_dict, di, clf2):\n",
        "    \n",
        "    verdict_dict = {}\n",
        "    \n",
        "    for i in range(1,2000):\n",
        "        for sub_id in pred_dict:\n",
        "        \n",
        "            if i >= len(pred_dict[sub_id]):\n",
        "                verdict_dict[sub_id].append(verdict_dict[sub_id][-1])\n",
        "            else:\n",
        "                seen = np.array(pred_dict[sub_id][:i])\n",
        "                avg = np.mean(seen)\n",
        "                sd = np.std(seen)\n",
        "                top_n = int(round((20*i)/100))\n",
        "                topn_avg = np.mean(np.sort(seen)[top_n:])\n",
        "                bottomn_avg = np.mean(np.sort(seen)[:top_n+1])\n",
        "                diff = topn_avg - bottomn_avg\n",
        "                n_texts = (i-1)/(1999-1)\n",
        "\n",
        "                x = np.array([n_texts,avg,sd,topn_avg,diff])\n",
        "                x = x.reshape(1,-1)\n",
        "                verdict = clf2.predict(x)\n",
        "\n",
        "                if sub_id in verdict_dict:\n",
        "                    verdict_dict[sub_id].append(verdict)\n",
        "                else:\n",
        "                    verdict_dict[sub_id] = [verdict]\n",
        "                \n",
        "    return verdict_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L7zFnpCuHbwD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(verdict_dict, o):\n",
        "    \n",
        "    # create dataframe to hold the data necessary for the final calculations\n",
        "    cols = [\"subject\", \"true_risk\", \"risk_decision\", \"delay\", \"erde\"]\n",
        "\n",
        "    df_final = pd.DataFrame(index = range(94),columns = cols)\n",
        "\n",
        "    count = 0\n",
        "    with open(\"/content/drive/My Drive/risk_test_users.txt\", 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            df_final.iloc[count]['subject'] = line.split('\\t')[0].strip()\n",
        "            df_final.iloc[count]['true_risk'] = float(line.split('\\t')[1].strip())\n",
        "            count += 1\n",
        "\n",
        "\n",
        "    # here i put the risk_decision and delay in the dataframe  \n",
        "    for key in verdict_dict:\n",
        "        sub_row = df_final.index[df_final['subject'] == key].tolist()[0]\n",
        "\n",
        "        df_final.iloc[sub_row,2] = verdict_dict[key][-1]\n",
        "\n",
        "        if (df_final.iloc[sub_row,2] == 1) & (df_final.iloc[sub_row,1] == 1):\n",
        "            df_final.iloc[sub_row,3] = verdict_dict[key].index(1)\n",
        "\n",
        "    #extract the data\n",
        "    risk_d = df_final['risk_decision']\n",
        "    t_risk = df_final['true_risk']\n",
        "    k = df_final['delay']\n",
        "    erde = df_final['erde']\n",
        "\n",
        "    # Count of how many true positives there are\n",
        "    true_pos = len(df_final[t_risk==1])\n",
        "\n",
        "    # Count of how many positive cases the system decided there were\n",
        "    pos_decisions = len(df_final[risk_d==1])\n",
        "\n",
        "    # Count of how many of them are actually true positive cases\n",
        "    pos_hits = len(df_final[(t_risk==1) & (risk_d==1)])\n",
        "\n",
        "    # Total count of users\n",
        "    total_users = len(df_final)\n",
        "\n",
        "    # ERDE calculations\n",
        "    for i in range(total_users):\n",
        "        if(risk_d[i] == 1 and t_risk[i] == 0):\n",
        "            erde.iloc[i] = float(true_pos)/total_users\n",
        "        elif(risk_d[i] == 0 and t_risk[i] == 1):\n",
        "            erde.iloc[i] = 1.0\n",
        "        elif(risk_d[i] == 1 and t_risk[i] == 1):\n",
        "            erde.iloc[i] = 1.0 - (1.0/(1.0+np.exp(k[i]-o)))\n",
        "        elif(risk_d[i] == 0 and t_risk[i] == 0):\n",
        "            erde.iloc[i] = 0.0\n",
        "\n",
        "    # Calculation of F1, Precision, Recall and global ERDE\n",
        "    precision = float(pos_hits)/pos_decisions\n",
        "    recall = float(pos_hits)/true_pos\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    erde_global = erde.mean() * 100\n",
        "\n",
        "    #indiv_erde = df_final.iloc[:,['subject','erde']]\n",
        "    #print (indiv_erde.to_string())\n",
        "    print ('Global ERDE (with o = %d): %.2f' % (o, erde_global), '%')\n",
        "    print ('F1: %.2f' % F1)\n",
        "    print ('Precision: %.2f' % precision)\n",
        "    print ('Recall: %.2f' % recall)\n",
        "    return df_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSDKTkWMHtrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read data, clean up texts\n",
        "\n",
        "df_train = read_data('/content/drive/My Drive/training.csv')\n",
        "df_test = read_data('/content/drive/My Drive/test.csv')\n",
        "\n",
        "#df_train = df_train.sample(frac=0.0005)\n",
        "#df_test = df_test.sample(frac=0.001)\n",
        "\n",
        "docs_train = df_train['Title'] + df_train['Text']\n",
        "Y_train = df_train[\"Diagnosis\"].values\n",
        "\n",
        "docs_test = df_test['Title'] + df_test['Text']\n",
        "Y_test = df_test[\"Diagnosis\"].values\n",
        "\n",
        "docs_train = docs_train.apply(cleanText)\n",
        "docs_test = docs_test.apply(cleanText)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dtyjrTmUqzWc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "elmo = ElmoEmbedder()\n",
        "\n",
        "X_train = get_vectors(docs_train, elmo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y69xPjk6nuFg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_text_length = 100\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_text_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDVMkZTsdmsl",
        "colab_type": "code",
        "outputId": "4c770595-ee0c-44c3-a44a-b73e60eb1129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "# create Keras classifier\n",
        "\n",
        "embedding_dim = 1024\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(100, 1024)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 100)               450000    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 450,101\n",
            "Trainable params: 450,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NCwmCxLTedvK",
        "colab_type": "code",
        "outputId": "9b881056-bb98-4db2-9938-8be08e604e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "filepath=\"/content/drive/My Drive/elmo_toy.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint, EarlyStopping(monitor='val_loss',min_delta=0.0001)]\n",
        "model.fit(X_train, Y_train, validation_split=0.2, callbacks=callbacks_list, epochs=5, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 66 samples, validate on 17 samples\n",
            "Epoch 1/5\n",
            "66/66 [==============================] - 2s 30ms/step - loss: 0.6482 - acc: 0.6515 - val_loss: 0.4148 - val_acc: 0.9412\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.94118, saving model to /content/drive/My Drive/elmo_toy.hdf5\n",
            "Epoch 2/5\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.4480 - acc: 0.8485 - val_loss: 0.2691 - val_acc: 0.9412\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.94118\n",
            "Epoch 3/5\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.4047 - acc: 0.8485 - val_loss: 0.2358 - val_acc: 0.9412\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.94118\n",
            "Epoch 4/5\n",
            "66/66 [==============================] - 1s 13ms/step - loss: 0.3812 - acc: 0.8485 - val_loss: 0.2478 - val_acc: 0.9412\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.94118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fec351d80b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "4xAYkcNyJNVO",
        "colab_type": "code",
        "outputId": "afaa4855-5203-45fb-bdf5-2ab666e9c9cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('/content/drive/My Drive/Models/elmo_model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6ZwyOg1OIOzQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sort texts by date and make predictions for both training and test set\n",
        "df_train['Text'] = df_train['Text'].apply(cleanText)\n",
        "df_test['Text'] = df_test['Text'].apply(cleanText)\n",
        "\n",
        "dict_train = sort_by_date(df_train)\n",
        "dict_test = sort_by_date(df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1L6E609dY2H0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_dict_train = predict_texts(dict_train, model)\n",
        "pred_dict_test = predict_texts(dict_test, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hsywlbceJtX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "texts = pd.Series([\"This is document number one. Not so interesting.\"])\n",
        "elmo = ElmoEmbedder(cuda_device = 0)\n",
        "x_texts = get_vectors(texts, elmo)\n",
        "x_texts = sequence.pad_sequences(x_texts, maxlen=100)\n",
        "if len(texts) == 1:\n",
        "    scores = model.predict(x_texts)[0]\n",
        "else:\n",
        "    scores = model.predict(x_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyyQppCYKsUq",
        "colab_type": "code",
        "outputId": "2e880f45-a9fd-4113-ee0b-b7385064247a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.08282965], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "QjNHjPY2IR8z",
        "colab_type": "code",
        "outputId": "83845387-9e02-45df-ded2-e8da668ed601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# get the feature vectors and train the person classifier\n",
        "\n",
        "x_train, y_train = create_train_vectors(pred_dict_train, dict_train)\n",
        "\n",
        "clf2 = LogisticRegression(max_iter=2000, class_weight = 'balanced',solver='saga')\n",
        "clf2.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "          fit_intercept=True, intercept_scaling=1, max_iter=2000,\n",
              "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
              "          solver='saga', tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "CtJqVMSJIU7g",
        "colab_type": "code",
        "outputId": "62c67b57-7de8-473c-fdde-06c2f5162803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_test, y_test = create_train_vectors(pred_dict_test, dict_test)\n",
        "clf2.score(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6428571428571429"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "9Y1TLH55IYA6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# predict test set and evaluate\n",
        "\n",
        "verdict_dict = predict_test_vectors(pred_dict_test, dict_test, clf2)\n",
        "df_final = evaluate(verdict_dict, 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s5uu0qK4Ia29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_final"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}